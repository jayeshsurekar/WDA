#pip install nltk---->has to be done in command prompt
#import nltk---->has to be done in python shell
#nltk.download('stopwords')---->has to be done in python shell
from bs4 import BeautifulSoup
import requests
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

# Download stopwords and initialize stemmer
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

# Retrieve webpage content
url = 'https://www.yahoo.com/?guccounter=1'
response = requests.get(url)

# Parse HTML and extract meta tags
soup = BeautifulSoup(response.content, 'html.parser')
meta_tags = soup.find_all('meta')

# Extract meta information
title = ''
description = ''
keywords = []

for tag in meta_tags:
        if tag.get('property')=='og:title':
            title=tag.get('content')
            
        if tag.get('property')=='og:description':
            discription=tag.get('content')
            
        if tag.get('name')=='keywords':
            keywords=tag.get('content')

# Preprocess content
content = soup.get_text()
tokens = word_tokenize(content)
filtered_tokens = [token.lower()for token in tokens if token.lower() not in stop_words]
stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]
preprocessed_content = ' '.join(stemmed_tokens)

# Print results
print('Title:', title,'\n')
print('Description:', discription,'\n')
print('keywords:', keywords,'\n')
print('Preprocessed content:', preprocessed_content[0:100],'/n')
